{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7svw4PXXvP9",
        "outputId": "9abdef77-ebb4-4e02-943a-8250db3f4fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram Feature Extraction"
      ],
      "metadata": {
        "id": "Z6P-J8lTVgip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubb6G2C8mlza"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to compute the histogram for each channel manually\n",
        "def compute_histogram(patch, bins=8):\n",
        "    hist_vector = np.zeros(bins)\n",
        "    bin_width = 256 // bins\n",
        "\n",
        "    # Flatten the patch for simpler looping\n",
        "    patch_flat = patch.flatten()\n",
        "\n",
        "    # Count occurrences of pixel values in each bin\n",
        "    for pixel_value in patch_flat:\n",
        "        bin_index = pixel_value // bin_width\n",
        "        hist_vector[bin_index] += 1\n",
        "\n",
        "    # Normalize the histogram by the total number of pixels in the patch\n",
        "    hist_vector /= len(patch_flat)\n",
        "    return hist_vector\n",
        "\n",
        "# Function to extract a 24-dim color histogram from a patch\n",
        "def color_histogram(patch, bins=8):\n",
        "    hist_r = compute_histogram(patch[:, :, 0], bins=bins)\n",
        "    hist_g = compute_histogram(patch[:, :, 1], bins=bins)\n",
        "    hist_b = compute_histogram(patch[:, :, 2], bins=bins)\n",
        "\n",
        "    # Concatenate histograms to form a 24-dim vector\n",
        "    hist_vector = np.concatenate([hist_r, hist_g, hist_b])\n",
        "    return hist_vector\n",
        "\n",
        "# Function to extract patches and their histograms from an image\n",
        "def extract_histogram_features(image, patch_size=32):\n",
        "    h, w, _ = image.shape\n",
        "    features = []\n",
        "\n",
        "    for i in range(0, h, patch_size):\n",
        "        for j in range(0, w, patch_size):\n",
        "            patch = image[i:i+patch_size, j:j+patch_size]\n",
        "            if patch.shape[:2] == (patch_size, patch_size):  # Ensure patch size matches\n",
        "                hist_vector = color_histogram(patch)\n",
        "                features.append(hist_vector)\n",
        "\n",
        "    return np.array(features)  # Stack all patch histograms for the image\n",
        "\n",
        "# Function to process all images in a folder and save their features\n",
        "def process_image_folder(image_folder, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    for image_name in os.listdir(image_folder):\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Extract 24-dimensional histogram features\n",
        "        features = extract_histogram_features(image)\n",
        "\n",
        "        # Save features in the corresponding folder\n",
        "        save_path = os.path.join(save_folder, f\"{os.path.splitext(image_name)[0]}_features.npy\")\n",
        "        np.save(save_path, features)\n",
        "\n",
        "# Example usage\n",
        "# class_folders = ['class1', 'class2', 'class3']\n",
        "# for class_folder in class_folders:\n",
        "#     process_image_folder(f\"./images/{class_folder}\", f\"./features/{class_folder}\")\n",
        "\n",
        "\n",
        "#process_image_folder(f\"/content/drive/MyDrive/group07(3)/group07/train/\",f\"/content/drive/MyDrive/group07(3)/group07/arch_new\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CyFD38JXtp8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "3a26e968-01a2-4874-c623-963416379ed8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f1fa7c06f60>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/group07_scene/group07/train/arch/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprocess_image_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/group07/features/arch_new/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory does not exist: {image_folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2c4f88c42ef9>\u001b[0m in \u001b[0;36mprocess_image_folder\u001b[0;34m(image_folder, save_folder)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Extract 24-dimensional histogram features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_histogram_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Save features in the corresponding folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2c4f88c42ef9>\u001b[0m in \u001b[0;36mextract_histogram_features\u001b[0;34m(image, patch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure patch size matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mhist_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2c4f88c42ef9>\u001b[0m in \u001b[0;36mcolor_histogram\u001b[0;34m(patch, bins)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolor_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mhist_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mhist_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mhist_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2c4f88c42ef9>\u001b[0m in \u001b[0;36mcompute_histogram\u001b[0;34m(patch, bins)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpixel_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatch_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbin_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_value\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbin_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mhist_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbin_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Normalize the histogram by the total number of pixels in the patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/train/arch/\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features/arch_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/test/arch/\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features_test/arch_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")"
      ],
      "metadata": {
        "id": "JrS_wL4HN-wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2im9CJHVKnT"
      },
      "outputs": [],
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/train/forest_path\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features/forest_path_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/test/forest_path\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features_test/forest_path_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")\n"
      ],
      "metadata": {
        "id": "Mne0dWIKOOyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlJ6G983Xh3s"
      },
      "outputs": [],
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/train/highway/\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features/highway_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"/content/drive/MyDrive/group07_scene/group07/test/highway/\"\n",
        "if os.path.exists(image_folder):\n",
        "    process_image_folder(image_folder, \"/content/drive/MyDrive/group07/features_test/highway_new/\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {image_folder}\")"
      ],
      "metadata": {
        "id": "2AsDbsJgOgyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering in 32 clusters"
      ],
      "metadata": {
        "id": "amqE5CwmVuLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to compute Euclidean distance between two points\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "# K-means clustering algorithm implementation from scratch\n",
        "def kmeans_clustering(data, k, max_iters=3):\n",
        "    # Randomly initialize cluster centroids\n",
        "    centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
        "\n",
        "    # Initialize cluster labels for each point\n",
        "    labels = np.zeros(data.shape[0])\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        # Step 1: Assign each point to the nearest centroid\n",
        "        for i in range(data.shape[0]):\n",
        "            distances = np.array([euclidean_distance(data[i], centroid) for centroid in centroids])\n",
        "            labels[i] = np.argmin(distances)\n",
        "\n",
        "        # Step 2: Update the centroids by calculating the mean of the points in each cluster\n",
        "        new_centroids = np.zeros(centroids.shape)\n",
        "        for i in range(k):\n",
        "            points_in_cluster = data[labels == i]\n",
        "            if len(points_in_cluster) > 0:\n",
        "                new_centroids[i] = np.mean(points_in_cluster, axis=0)\n",
        "\n",
        "        # If the centroids don't change, we have converged\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, labels"
      ],
      "metadata": {
        "id": "low42nWOvgk2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_feature_vectors(feature_folder):\n",
        "    all_features = []\n",
        "    for class_folder in os.listdir(feature_folder):\n",
        "        class_path = os.path.join(feature_folder, class_folder)\n",
        "        for feature_file in os.listdir(class_path):\n",
        "            feature_path = os.path.join(class_path, feature_file)\n",
        "            features = np.load(feature_path)\n",
        "            all_features.append(features)\n",
        "\n",
        "    return np.vstack(all_features)  # Combine all features into one array\n",
        "\n",
        "# Example usage of kmeans\n",
        "train_features = collect_feature_vectors(\"/content/drive/MyDrive/group07/features/\")\n",
        "k = 32\n",
        "centroids, labels = kmeans_clustering(train_features, k)\n",
        "print(len(centroids))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "sO0rtM8xx7bD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7b70ca-45aa-4853-bf93-36843149ba33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "194877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = collect_feature_vectors(\"/content/drive/MyDrive/group07/features_test/\")\n",
        "k = 32\n",
        "centroids_test, labels_test = kmeans_clustering(test_features, k)\n",
        "print(len(centroids_test))\n",
        "print(len(labels_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTL_RncKO5T5",
        "outputId": "286edb51-9265-4b0b-d7f0-53920bf2a605"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "169332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BOVW Features Extraction\n",
        "\n"
      ],
      "metadata": {
        "id": "WrIKH8rgVziA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8HWyFmgpoubi"
      },
      "outputs": [],
      "source": [
        "# Function to assign histogram vectors to clusters and form BoVW representation\n",
        "def bovw_representation(image_features, centroids):\n",
        "    k = centroids.shape[0]\n",
        "    bovw_vector = np.zeros(k)\n",
        "\n",
        "    # Assign each feature to the closest centroid\n",
        "    for feature in image_features:\n",
        "        distances = np.array([euclidean_distance(feature, centroid) for centroid in centroids])\n",
        "        nearest_centroid = np.argmin(distances)\n",
        "        bovw_vector[nearest_centroid] += 1\n",
        "\n",
        "    # Normalize the BoVW vector\n",
        "    bovw_vector /= len(image_features)\n",
        "    return bovw_vector\n",
        "\n",
        "# Generate BoVW representations for all images in the test set\n",
        "def generate_bovw_for_images(feature_folder, centroids, save_folder):\n",
        "    bovw_features=[]\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "    for class_folder in os.listdir(feature_folder):\n",
        "        class_path = os.path.join(feature_folder, class_folder)\n",
        "        for feature_file in os.listdir(class_path):\n",
        "            feature_path = os.path.join(class_path, feature_file)\n",
        "            features = np.load(feature_path)\n",
        "\n",
        "            # Generate BoVW representation\n",
        "            bovw_vector = bovw_representation(features, centroids)\n",
        "            bovw_features.append(bovw_vector)\n",
        "\n",
        "            # Save the BoVW vector\n",
        "            save_path = os.path.join(save_folder, f\"{os.path.splitext(feature_file)[0]}_bovw.npy\")\n",
        "            np.save(save_path, bovw_vector)\n",
        "    return bovw_features\n",
        "\n",
        "\n",
        "# Example usage\n",
        "bovw_features_train=[]\n",
        "bovw_features_train=generate_bovw_for_images(\"/content/drive/MyDrive/group07/features/\", centroids, \"/content/drive/MyDrive/group07/bovw\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(bovw_features_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1uc2KfZIarE",
        "outputId": "eb7e4def-b21f-443d-93ca-997822e6236e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bovw_features_test=[]\n",
        "bovw_features_test=generate_bovw_for_images(\"/content/drive/MyDrive/group07/features_test/\", centroids_test, \"/content/drive/MyDrive/group07/bovw_test\")"
      ],
      "metadata": {
        "id": "_t_X9vOfPShr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bovw_features_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kraDvsp5QYCG",
        "outputId": "5cfa19ba-3f46-4dfe-9e8c-b1fc616f4431"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def collect_bovw_vectors(bovw_folder):\n",
        "    bovw_vectors = []\n",
        "\n",
        "    # Iterate through all files in the bovw folder\n",
        "    for file_name in os.listdir(bovw_folder):\n",
        "        if file_name.endswith(\".npy\"):  # Check if the file is an .npy file\n",
        "            file_path = os.path.join(bovw_folder, file_name)\n",
        "\n",
        "            # Load the BoVW vector and append it to the list\n",
        "            bovw_vector = np.load(file_path)\n",
        "            bovw_vectors.append(bovw_vector)\n",
        "\n",
        "    return np.array(bovw_vectors)\n",
        "\n",
        "# Example usage\n",
        "# bovw_folder = \"/content/drive/MyDrive/group07/features/bovw\"\n",
        "# bovw_vectors = collect_bovw_vectors(bovw_folder)\n",
        "\n",
        "# Now bovw_vectors is a list of all BoVW vectors from the folder\n"
      ],
      "metadata": {
        "id": "ZjPWGFzmIvtW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using bovw features"
      ],
      "metadata": {
        "id": "6wxPxWRHV-it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gaussian_density(x, mean, cov):\n",
        "#     n = len(x)\n",
        "#     cov_inv = np.linalg.inv(cov)\n",
        "#     norm_const = 1.0 / (np.power((2 * np.pi), n / 2) * np.sqrt(np.linalg.det(cov)))\n",
        "#     x_mean = x - mean\n",
        "#     return norm_const * np.exp(-0.5 * np.dot(np.dot(x_mean.T, cov_inv), x_mean))\n",
        "\n",
        "def gaussian_density(x, mean, cov):\n",
        "    n = len(x)\n",
        "    # Add regularization to covariance matrix\n",
        "    cov = cov + np.eye(cov.shape[0]) * 1e-3\n",
        "    cov_inv = np.linalg.inv(cov)\n",
        "    norm_const = 1.0 / (np.power((2 * np.pi), n / 2) * np.sqrt(np.linalg.det(cov)))\n",
        "    x_mean = x - mean\n",
        "    return norm_const * np.exp(-0.5 * (x_mean.T @ cov_inv @ x_mean))"
      ],
      "metadata": {
        "id": "P9smXr1yuKqr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_gmm_params(X, n_components):\n",
        "    centroids, labels = kmeans_clustering(X, n_components)\n",
        "    means = centroids\n",
        "    covariances = np.array([np.cov(X[labels == i].T) for i in range(n_components)])\n",
        "    priors = np.array([np.mean(labels == i) for i in range(n_components)])\n",
        "    return means, covariances, priors"
      ],
      "metadata": {
        "id": "LmKzHpmquLPd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def gmm_em(X, n_components, max_iter=5):\n",
        "#     n_samples, n_features = X.shape\n",
        "#     means, covariances, priors = initialize_gmm_params(X, n_components)\n",
        "\n",
        "#     for _ in range(max_iter):\n",
        "#         # E-step: Calculate responsibilities (gamma)\n",
        "#         responsibilities = np.zeros((n_samples, n_components))\n",
        "#         for i in range(n_components):\n",
        "#             for j in range(n_samples):\n",
        "#                 responsibilities[j, i] = priors[i] * gaussian_density(X[j], means[i], covariances[i])\n",
        "\n",
        "#         responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
        "\n",
        "#         # M-step: Update parameters\n",
        "#         N_k = responsibilities.sum(axis=0)\n",
        "#         means = np.array([np.dot(responsibilities[:, k], X) / N_k[k] for k in range(n_components)])\n",
        "#         covariances = np.array([np.dot((responsibilities[:, k] * (X - means[k]).T), (X - means[k])) / N_k[k] for k in range(n_components)])\n",
        "#         priors = N_k / n_samples\n",
        "\n",
        "#     return means, covariances, priors\n",
        "\n",
        "def gmm_em(X, n_components, max_iter=5,reg=1e-6):\n",
        "    n_samples, n_features = X.shape\n",
        "    means, covariances, priors = initialize_gmm_params(X, n_components)\n",
        "\n",
        "    # Calculate labels here to access within the function\n",
        "    _, labels = kmeans_clustering(X, n_components)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # E-step: Calculate responsibilities (gamma)\n",
        "        responsibilities = np.zeros((n_samples, n_components))\n",
        "        for i in range(n_components):\n",
        "            try:\n",
        "                for j in range(n_samples):\n",
        "                    responsibilities[j, i] = priors[i] * gaussian_density(X[j], means[i], covariances[i])\n",
        "            except np.linalg.LinAlgError:\n",
        "                print(f\"Error in cluster {i}\")\n",
        "                print(f\"Number of points in cluster {i}: {len(X[labels == i])}\")\n",
        "                raise\n",
        "\n",
        "        responsibilities /= (responsibilities.sum(axis=1, keepdims=True)+1e-10)\n",
        "\n",
        "        # M-step: Update parameters\n",
        "        N_k = responsibilities.sum(axis=0)\n",
        "        means = np.array([np.dot(responsibilities[:, k], X) / N_k[k] for k in range(n_components)])\n",
        "        covariances = np.array([np.dot((responsibilities[:, k] * (X - means[k]).T), (X - means[k])) / N_k[k] for k in range(n_components)])\n",
        "        for i in range(n_components):\n",
        "            covariances[i] += reg * np.eye(n_features)\n",
        "        priors = N_k / n_samples\n",
        "\n",
        "    return means, covariances, priors"
      ],
      "metadata": {
        "id": "H5JXygJVuA8k"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def classify(X_test, gmm_params_by_class):\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        posteriors = []\n",
        "        for class_label, (means, covariances, priors) in gmm_params_by_class.items():\n",
        "            posterior = 0\n",
        "            for i in range(len(means)):\n",
        "                likelihood = gaussian_density(x, means[i], covariances[i])\n",
        "                posterior += priors[i] * likelihood\n",
        "            posteriors.append(posterior)\n",
        "        predictions.append(np.argmax(posteriors))\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Fit GMM for each class\n",
        "def fit_gmm_for_all_classes(X_class1, X_class2, X_class3, n_components):\n",
        "    gmm_params_by_class = {}\n",
        "\n",
        "    # Class 1\n",
        "    means1, covariances1, priors1 = gmm_em(X_class1, n_components)\n",
        "    gmm_params_by_class[0] = (means1, covariances1, priors1)\n",
        "\n",
        "    # Class 2\n",
        "    means2, covariances2, priors2 = gmm_em(X_class2, n_components)\n",
        "    gmm_params_by_class[1] = (means2, covariances2, priors2)\n",
        "\n",
        "    # Class 3\n",
        "    means3, covariances3, priors3 = gmm_em(X_class3, n_components)\n",
        "    gmm_params_by_class[2] = (means3, covariances3, priors3)\n",
        "\n",
        "    return gmm_params_by_class"
      ],
      "metadata": {
        "id": "7jti6pIdted_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "def run_experiment(X_class1, X_class2, X_class3, X_test, y_test, n_mixtures_list):\n",
        "    results = {}\n",
        "\n",
        "    for n_mixtures in n_mixtures_list:\n",
        "        # Fit GMM for all three classes\n",
        "        gmm_params_by_class = fit_gmm_for_all_classes(X_class1, X_class2, X_class3, n_mixtures)\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = classify(X_test, gmm_params_by_class)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Print out the results for this experiment\n",
        "        print(f\"GMM with {n_mixtures} mixtures:\")\n",
        "        print(f\"  Accuracy = {accuracy:.4f}\")\n",
        "        print(classification_report(y_test, y_pred))  # Display full report\n",
        "        print(f\"Confusion Matrix:\\n{cm}\\n\")\n",
        "\n",
        "        # Store results for this number of mixtures\n",
        "        results[n_mixtures] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"classification_report\": report,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6eH0HRkKteWn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array([0] * 50 + [1] * 50 + [2] * 50)"
      ],
      "metadata": {
        "id": "o_jUUX4IdqHj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bovw_features_train = np.array(bovw_features_train)\n",
        "bovw_features_test = np.array(bovw_features_test)"
      ],
      "metadata": {
        "id": "7F3B2EGEeNHI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_mixtures_list = [1,2,4,8,16]\n",
        "results = run_experiment(bovw_features_train[:50], bovw_features_train[50:100], bovw_features_train[100:], bovw_features_test, y_test, n_mixtures_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxRllZiHt_yW",
        "outputId": "ed390725-c4be-49c3-8d21-d151f009bd09"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 1 mixtures:\n",
            "  Accuracy = 0.2867\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.56      0.39        50\n",
            "           1       0.35      0.14      0.20        50\n",
            "           2       0.22      0.16      0.18        50\n",
            "\n",
            "    accuracy                           0.29       150\n",
            "   macro avg       0.29      0.29      0.26       150\n",
            "weighted avg       0.29      0.29      0.26       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28  7 15]\n",
            " [29  7 14]\n",
            " [36  6  8]]\n",
            "\n",
            "GMM with 2 mixtures:\n",
            "  Accuracy = 0.3267\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.76      0.46        50\n",
            "           1       0.47      0.16      0.24        50\n",
            "           2       0.17      0.06      0.09        50\n",
            "\n",
            "    accuracy                           0.33       150\n",
            "   macro avg       0.32      0.33      0.26       150\n",
            "weighted avg       0.32      0.33      0.26       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[38  6  6]\n",
            " [33  8  9]\n",
            " [44  3  3]]\n",
            "\n",
            "GMM with 4 mixtures:\n",
            "  Accuracy = 0.2733\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.36      0.31        50\n",
            "           1       0.33      0.14      0.20        50\n",
            "           2       0.26      0.32      0.29        50\n",
            "\n",
            "    accuracy                           0.27       150\n",
            "   macro avg       0.29      0.27      0.26       150\n",
            "weighted avg       0.29      0.27      0.26       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[18  6 26]\n",
            " [23  7 20]\n",
            " [26  8 16]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-eb9e87e8685a>:4: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  covariances = np.array([np.cov(X[labels == i].T) for i in range(n_components)])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py:2180: RuntimeWarning: invalid value encountered in det\n",
            "  r = _umath_linalg.det(a, signature=signature)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-42-eb9e87e8685a>:4: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  covariances = np.array([np.cov(X[labels == i].T) for i in range(n_components)])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 8 mixtures:\n",
            "  Accuracy = 0.3333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      1.00      0.50        50\n",
            "           1       0.00      0.00      0.00        50\n",
            "           2       0.00      0.00      0.00        50\n",
            "\n",
            "    accuracy                           0.33       150\n",
            "   macro avg       0.11      0.33      0.17       150\n",
            "weighted avg       0.11      0.33      0.17       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[50  0  0]\n",
            " [50  0  0]\n",
            " [50  0  0]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py:2180: RuntimeWarning: invalid value encountered in det\n",
            "  r = _umath_linalg.det(a, signature=signature)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 16 mixtures:\n",
            "  Accuracy = 0.3333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      1.00      0.50        50\n",
            "           1       0.00      0.00      0.00        50\n",
            "           2       0.00      0.00      0.00        50\n",
            "\n",
            "    accuracy                           0.33       150\n",
            "   macro avg       0.11      0.33      0.17       150\n",
            "weighted avg       0.11      0.33      0.17       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[50  0  0]\n",
            " [50  0  0]\n",
            " [50  0  0]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification using histogram features"
      ],
      "metadata": {
        "id": "P_qAKk9Rq8MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_image_likelihood(image_features, gmm_params_by_class):\n",
        "#     likelihoods_by_class = {}\n",
        "#     for class_label, (means, covariances, priors) in gmm_params_by_class.items():\n",
        "#         image_likelihood = 1.0\n",
        "#         for patch_features in image_features:\n",
        "#             patch_likelihood = 0\n",
        "#             for i in range(len(means)):\n",
        "#                 likelihood = gaussian_density(patch_features, means[i], covariances[i])\n",
        "#                 patch_likelihood += priors[i] * likelihood\n",
        "#             image_likelihood *= patch_likelihood\n",
        "#         likelihoods_by_class[class_label] = image_likelihood\n",
        "#     return likelihoods_by_class\n",
        "\n",
        "def calculate_image_log_likelihood(image_features, gmm_params_by_class):\n",
        "    log_likelihoods_by_class = {}\n",
        "    for class_label, (means, covariances, priors) in gmm_params_by_class.items():\n",
        "        image_log_likelihood = 0  # Initialize with 0 for summation\n",
        "        for patch_features in image_features:\n",
        "            patch_log_likelihood = -np.inf  # Initialize with -inf for max operation\n",
        "            for i in range(len(means)):\n",
        "                likelihood = gaussian_density(patch_features, means[i], covariances[i])\n",
        "\n",
        "                # Calculate log-likelihood and handle potential zero likelihood\n",
        "                log_likelihood_component = np.log(priors[i]) + np.log(likelihood) if likelihood > 0 else -np.inf\n",
        "\n",
        "                patch_log_likelihood = max(patch_log_likelihood, log_likelihood_component)  # Use max for numerical stability\n",
        "\n",
        "            image_log_likelihood += patch_log_likelihood  # Sum log-likelihoods of patches\n",
        "\n",
        "        log_likelihoods_by_class[class_label] = image_log_likelihood\n",
        "    return log_likelihoods_by_class"
      ],
      "metadata": {
        "id": "vl0PuCWq1wUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gmms(feature_folder, n_components=3):\n",
        "    gmm_params_by_class = {}\n",
        "    class_labels = {'arch_new': 0, 'forest_path_new': 1, 'highway_new': 2}\n",
        "\n",
        "    for class_name, class_label in class_labels.items():\n",
        "        class_path = os.path.join(feature_folder, class_name)\n",
        "        class_features = []\n",
        "        for feature_file in os.listdir(class_path):\n",
        "            feature_path = os.path.join(class_path, feature_file)\n",
        "            features = np.load(feature_path)\n",
        "            class_features.extend(features)\n",
        "        means, covariances, priors = gmm_em(np.array(class_features), n_components)\n",
        "        gmm_params_by_class[class_label] = (means, covariances, priors)\n",
        "\n",
        "    return gmm_params_by_class"
      ],
      "metadata": {
        "id": "nBlJ99TTw3jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image_features, gmm_params_by_class):\n",
        "    log_likelihoods = calculate_image_log_likelihood(image_features, gmm_params_by_class)\n",
        "    predicted_class = max(log_likelihoods, key=log_likelihoods.get)\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "LzECedEew960"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "def run_experiment(feature_folder_train, feature_folder_test, n_components=3):\n",
        "    # Train GMMs\n",
        "    gmm_params_by_class = train_gmms(feature_folder_train, n_components)\n",
        "\n",
        "    # Load test images and classify\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    class_labels = {'arch_new': 0, 'forest_path_new': 1, 'highway_new': 2}\n",
        "\n",
        "    for class_name, class_label in class_labels.items():\n",
        "        class_path = os.path.join(feature_folder_test, class_name)\n",
        "        for feature_file in os.listdir(class_path):\n",
        "            if feature_file.endswith(\".npy\"):  # Load .npy files\n",
        "                feature_path = os.path.join(class_path, feature_file)\n",
        "                image_features = np.load(feature_path)\n",
        "                predicted_class = classify_image(image_features, gmm_params_by_class)\n",
        "\n",
        "                y_true.append(class_label)\n",
        "                y_pred.append(predicted_class)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"GMM with {n_components} mixtures:\")\n",
        "    print(f\"  Accuracy = {accuracy:.4f}\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(f\"Confusion Matrix:\\n{cm}\\n\")\n",
        "\n",
        "    return accuracy, report, cm\n"
      ],
      "metadata": {
        "id": "mTvl2ViFxMcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_folder_train = \"/content/drive/MyDrive/group07/features/\"\n",
        "feature_folder_test = \"/content/drive/MyDrive/group07/features_test/\"\n",
        "n_components_list = [1, 2, 4]\n",
        "\n",
        "for n_components in n_components_list:\n",
        "    accuracy, report, cm = run_experiment(feature_folder_train, feature_folder_test, n_components)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnn5ctgMxZeP",
        "outputId": "bd3d3ec3-73cb-45ae-be91-204398bd4bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 1 mixtures:\n",
            "  Accuracy = 0.6600\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.46      0.49        50\n",
            "           1       0.77      0.94      0.85        50\n",
            "           2       0.63      0.58      0.60        50\n",
            "\n",
            "    accuracy                           0.66       150\n",
            "   macro avg       0.65      0.66      0.65       150\n",
            "weighted avg       0.65      0.66      0.65       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23 11 16]\n",
            " [ 2 47  1]\n",
            " [18  3 29]]\n",
            "\n",
            "GMM with 2 mixtures:\n",
            "  Accuracy = 0.7000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.62      0.60        50\n",
            "           1       0.73      0.96      0.83        50\n",
            "           2       0.87      0.52      0.65        50\n",
            "\n",
            "    accuracy                           0.70       150\n",
            "   macro avg       0.72      0.70      0.69       150\n",
            "weighted avg       0.72      0.70      0.69       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31 15  4]\n",
            " [ 2 48  0]\n",
            " [21  3 26]]\n",
            "\n",
            "GMM with 4 mixtures:\n",
            "  Accuracy = 0.6933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60        50\n",
            "           1       0.72      0.94      0.82        50\n",
            "           2       0.81      0.52      0.63        50\n",
            "\n",
            "    accuracy                           0.69       150\n",
            "   macro avg       0.71      0.69      0.68       150\n",
            "weighted avg       0.71      0.69      0.68       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31 14  5]\n",
            " [ 2 47  1]\n",
            " [20  4 26]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_components_list=[8,16,32]\n",
        "for n_components in n_components_list:\n",
        "    accuracy, report, cm = run_experiment(feature_folder_train, feature_folder_test, n_components)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vLr2ivcmx0pK",
        "outputId": "cc04ef3f-5334-45d8-8a36-448c48ce44cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 8 mixtures:\n",
            "  Accuracy = 0.6733\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.56      0.56        50\n",
            "           1       0.73      0.94      0.82        50\n",
            "           2       0.72      0.52      0.60        50\n",
            "\n",
            "    accuracy                           0.67       150\n",
            "   macro avg       0.67      0.67      0.66       150\n",
            "weighted avg       0.67      0.67      0.66       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28 13  9]\n",
            " [ 2 47  1]\n",
            " [20  4 26]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "<ipython-input-4-eb9e87e8685a>:4: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  covariances = np.array([np.cov(X[labels == i].T) for i in range(n_components)])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py:2180: RuntimeWarning: invalid value encountered in det\n",
            "  r = _umath_linalg.det(a, signature=signature)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMM with 16 mixtures:\n",
            "  Accuracy = 0.6333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.67      0.98      0.80        50\n",
            "           2       0.60      0.92      0.72        50\n",
            "\n",
            "    accuracy                           0.63       150\n",
            "   macro avg       0.42      0.63      0.51       150\n",
            "weighted avg       0.42      0.63      0.51       150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0 20 30]\n",
            " [ 0 49  1]\n",
            " [ 0  4 46]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "<ipython-input-4-eb9e87e8685a>:4: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  covariances = np.array([np.cov(X[labels == i].T) for i in range(n_components)])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-36134bf7fb0d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_components_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_components_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_folder_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_folder_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-edd92a0cdf71>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(feature_folder_train, feature_folder_test, n_components)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_folder_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_folder_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Train GMMs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgmm_params_by_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gmms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_folder_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Load test images and classify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3bc5b82bbb6a>\u001b[0m in \u001b[0;36mtrain_gmms\u001b[0;34m(feature_folder, n_components)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mclass_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mgmm_params_by_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e1deb2ac4c58>\u001b[0m in \u001b[0;36mgmm_em\u001b[0;34m(X, n_components, max_iter, reg)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Calculate labels here to access within the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-26229ec512cc>\u001b[0m in \u001b[0;36mkmeans_clustering\u001b[0;34m(data, k, max_iters)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Step 1: Assign each point to the nearest centroid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcentroid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-26229ec512cc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Step 1: Assign each point to the nearest centroid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcentroid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-26229ec512cc>\u001b[0m in \u001b[0;36meuclidean_distance\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Function to compute Euclidean distance between two points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# K-means clustering algorithm implementation from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2314\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5p-EFBVE2Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}